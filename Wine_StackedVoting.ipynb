{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Stacked Model with a Blender to Improve Wine Quality Estimation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'qualities': 7\n",
      "Qualities: [3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "Class counts: [   0    0    0   20  163 1457 2198  880  175    5]\n",
      "\n",
      "Number of instances: 4898 \n",
      "\n",
      "Class fractions: [ 0.    0.    0.    0.41  3.33 29.75 44.88 17.97  3.57  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"./winequality-white.csv\")\n",
    "data_columns = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \n",
    "\"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n",
    "wine_df = pd.read_csv(data_path, header=0, names=data_columns, sep=\";\")\n",
    "\n",
    "test_size=0.2\n",
    "train_df, test_df = train_test_split(wine_df, test_size=test_size)\n",
    "\n",
    "# data overview:\n",
    "#print(wine_df.describe())\n",
    "\n",
    "# which quality classes do we have? :\n",
    "qualities = wine_df[\"quality\"].unique()\n",
    "print(f\"Number of unique 'qualities': {len(qualities)}\")\n",
    "print(f\"Qualities: {sorted(qualities)}\")\n",
    "binc = np.bincount([q for q in wine_df[\"quality\"]])\n",
    "no_inst = len(wine_df)\n",
    "print(f\"\\nClass counts: {binc}\")\n",
    "print(f\"\\nNumber of instances: {no_inst} \")\n",
    "print(f\"\\nClass fractions: {np.round(binc/no_inst,4) * 100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToDo: !Refactoring: don't use pandas dataframes, where not necessary! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_dataframe(data_df, exempt_last_column=False, column_names_to_scale=None):\n",
    "    \"\"\"\n",
    "        Scales columns of a given data frame with a StandardScaler from Sklearn. \n",
    "        Input:\n",
    "            data_df : dataframe with numerical values to normalize\n",
    "            exempt_last_column : if true, column_names_to_scale will be ignored and all but the last column will be scaled.\n",
    "            column_names_to_scale : list of the names of the columns to be scaled\n",
    "\n",
    "        Output:\n",
    "            dataframe with columns scaled\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # quickfix (until refactoring out pandas..) for data_df as np.arrays instead of pd.DataFrame\n",
    "    if(isinstance(data_df, pd.DataFrame)):\n",
    "        data = data_df.to_numpy()\n",
    "    else:\n",
    "        data = data_df\n",
    "\n",
    "    if(exempt_last_column & (column_names_to_scale != None)):\n",
    "        raise UserWarning(\"exempt_last_column=True : your column_names_to_scale will be ignored!\")\n",
    "    if(exempt_last_column):\n",
    "        data_to_scale = data[:,:-1]\n",
    "        last_column = np.expand_dims(data[:,-1].astype(np.int_), axis=1)\n",
    "        data_scaled = np.append(scaler.fit_transform(data_to_scale), last_column, axis=1)\n",
    "        return pd.DataFrame(data_scaled)\n",
    "    elif(column_names_to_scale):\n",
    "        data_to_scale = data[column_names_to_scale]\n",
    "        data_scaled = scaler.fit_transform(data_to_scale)\n",
    "        df_temp = pd.DataFrame(data_scaled, columns=column_names_to_scale, index=data_df.index)\n",
    "        data_df[column_names_to_scale]= df_temp\n",
    "    else:\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        data_df = pd.DataFrame(data_scaled)\n",
    "    \n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Model definition from the WineDataset note book -- I don't know how to import this from another Jupyter notebook...\\nfrom torch import nn\\n\\nclass WineNetwork(nn.Module):\\n    def __init__(self):\\n        super(WineNetwork, self).__init__()\\n        self.linear_relu_stack = nn.Sequential(\\n            nn.Linear(11, 64),\\n            nn.ReLU(),\\n            nn.BatchNorm1d(64),\\n            nn.Dropout(p=0.2),\\n            nn.Linear(64, 128),\\n            nn.ReLU(),\\n            nn.BatchNorm1d(128),\\n            nn.Linear(128, 256),\\n            nn.ReLU(),\\n            nn.BatchNorm1d(256),\\n            nn.Linear(256, 10),\\n            nn.ReLU()\\n        )\\n\\n    def forward(self, x):\\n        logits = self.linear_relu_stack(x)\\n        return logits\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Model definition from the WineDataset note book -- I don't know how to import this from another Jupyter notebook...\n",
    "from torch import nn\n",
    "\n",
    "class WineNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WineNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train- and test-loop for the blender:\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    losses, nof_correct = 0, 0\n",
    "    for xx, y_true in dataloader:\n",
    "        y_pred = model(xx)\n",
    "        loss= loss_fn(y_pred, y_true)\n",
    "        losses += loss.item()\n",
    "        nof_correct += (y_pred.argmax(1) == y_true).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses, nof_correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    losses, no_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (X,y_true) in dataloader:\n",
    "            pred = model(X)\n",
    "            losses += loss_fn(pred, y_true).item()\n",
    "            no_correct += (pred.argmax(1)== y_true).sum().item()\n",
    "     \n",
    "    return losses, no_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Maps to Transform the Training-Data from the Disk - This is the First Layer of Stacking: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision-Tree Map:<br>\n",
    "Is used for the AdaBoost-Decision-Tree and for the plain decision-tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data using the decision-tree:\n",
    "\n",
    "import pickle\n",
    "\n",
    "def Wine_tree_map(data_df, tree_name, does_predict_proba=False):\n",
    "    \"\"\"\n",
    "    Maps a data-frame by using a pre-trained decision-tree.\n",
    "    Input:\n",
    "        data_df : Pandas data-frame to be mapped, containing (x, y_true)\n",
    "        tree_name : path / name of the pretrained tree\n",
    "        does_predict_proba : uses the predict_proba method if true, uses predict otherwize\n",
    "    Returns:\n",
    "        Pandas data-frame containing (y_pred, y_true)\n",
    "    \"\"\"\n",
    "    tree_model = pickle.load(open(tree_name, 'rb'))\n",
    "    X = data_df.to_numpy()\n",
    "    if(does_predict_proba):\n",
    "        # \"np.int_\" is \"long\" in numpy:\n",
    "        Z = np.append(tree_model.predict_proba(X[:,:-1]), np.expand_dims(X[:,-1].astype(np.int_), axis=1), axis=1)\n",
    "    else:\n",
    "        Z = np.append(np.expand_dims(tree_model.predict(X[:,:-1]).astype(np.int_), axis=1), np.expand_dims(X[:,-1].astype(np.int_), axis=1), axis=1)\n",
    "    \n",
    "    return pd.DataFrame(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural-Net Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the training data using the neural-net:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def WineNetwork_map(data_df, net_name):\n",
    "    \"\"\"\n",
    "    Maps a dataframe by applying a pre-trained neural net of type \"class WineNetwork\"\n",
    "    Input:\n",
    "        data_df : the pandas data-frame to be transformed, containing (x, y_true)\n",
    "        net_name : the path / name of the net-model of type WineNetwork to be loaded\n",
    "    Returns:\n",
    "        pandas data-frame containing (y_pred probatilities, y_true)\n",
    "    \"\"\"\n",
    "    class WineNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(WineNetwork, self).__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(11, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.Linear(128, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.Linear(256, 10),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            logits = self.linear_relu_stack(x)\n",
    "            return logits\n",
    "\n",
    "    net_model = WineNetwork()\n",
    "    net_model.load_state_dict(torch.load(net_name))\n",
    "    net_model.eval()\n",
    "\n",
    "    # add softmax for prediction of probabilities:\n",
    "    # softm = lambda x : np.exp(x)/np.sum(np.exp(x))\n",
    "    softm = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    X = torch.tensor(data_df.iloc[:,:-1].to_numpy(), dtype=torch.float32).detach()\n",
    "    Y = torch.tensor(data_df.iloc[:, -1].to_numpy(), dtype=torch.long).detach()\n",
    "\n",
    "    Z = torch.cat((softm(net_model(X)), Y.unsqueeze(dim=1)), dim=1).detach().numpy()\n",
    "    \n",
    "    return pd.DataFrame(Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm map:\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def SVM_RBF_map(data_df, svm_filename, use_one_hot_encoding=True):\n",
    "    \"\"\"\n",
    "    Maps a data-frame by using a pre-trained support-vector-machine.\n",
    "    Input:\n",
    "        data_df : Pandas data-frame to be mapped, containing (x, y_true)\n",
    "        svm_filename : path / name of the pretrained svm\n",
    "        use_one_hot_encoding : returns one-hot-encoded predictions (for the range of the wine categories 0..9)\n",
    "    Returns:\n",
    "        Pandas data-frame containing (y_pred, y_true)\n",
    "    \"\"\"\n",
    "    X = data_df.to_numpy()\n",
    "    svm = pickle.load(open(svm_filename, \"rb\"))\n",
    "    preds = svm.predict(X[:,:-1]).astype(np.int_)\n",
    "    if(use_one_hot_encoding):\n",
    "        ohe = LabelBinarizer()\n",
    "        ohe.fit(np.arange(10))\n",
    "        preds = ohe.transform(preds)\n",
    "    else:\n",
    "        preds = np.expand_dims(preds, axis=1)\n",
    "\n",
    "    Z = np.append(preds, np.expand_dims(X[:,-1].astype(np.int_), axis=1), axis=1)\n",
    "\n",
    "    return pd.DataFrame(Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch DataSet - to be used for batching with torch DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the boiler-plate triade: pandas.data-frame -> torch.data-set -> torch.data-loader\n",
    "\n",
    "# define torch.dataset: __init__(), __len__(), __getitem__()\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WineDataSet(Dataset):\n",
    "    def __init__(self, data_df, transform=None, target_transform=None):\n",
    "        self.data_df = data_df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.X = torch.tensor(self.data_df.iloc[:,:-1].to_numpy(), dtype=torch.float32)\n",
    "        self.Y = torch.tensor(self.data_df.iloc[:, -1].to_numpy(), dtype= torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        self.x = self.X[idx,:]\n",
    "        self.y = self.Y[idx]\n",
    "        if self.transform != None:\n",
    "            self.x = self.transform(self.x)\n",
    "        if self.target_transform != None:\n",
    "            self.y = self.target_transform(self.y)\n",
    "        return self.x, self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# LOAD AND PREPARE DATA: DATA FRESH LOAD - NOT for stacking:\\n\\nfrom pathlib import Path\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\n\\n\\n# DATA FRESH LOAD - NOT for stacking:\\ndata_path = Path(\"./winequality-white.csv\")\\ncolumn_names = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \\n\"pH\", \"sulphates\", \"alcohol\", \"quality\"]\\n#column_names_to_normalize = column_names[:-1]\\nwine_df = pd.read_csv(data_path, header=0, names=column_names, sep=\";\")\\n\\n# prepare data:\\nshuffle(wine_df, random_state=0)\\nscaled_wine_df = scale_dataframe(wine_df, exempt_last_column=True)\\n\\n# names/ paths of the pre-trained classifiers:\\n#WineNetwork_filename = \"model_640_0.001_369_64_SGD.pt\"\\nNET_FILENAME=\"model_635_0.001_255_64_SGD.pt\"\\nTREE_FILENAME= \\'AdaBoost_071_model.dct\\'\\n\\n# use the scaled data for the net:\\nnet_df = WineNetwork_map(scaled_wine_df, net_name=NET_FILENAME)\\nnet_df = scale_dataframe(net_df)\\n\\n# use the unscaled data for the tree:\\ntree_df = Wine_tree_map(wine_df, tree_name=TREE_FILENAME, does_predict_proba=True)\\ntree_df = scale_dataframe(tree_df)\\n\\n# combine net_df and tree_df to combined_data_df:\\ncombined_data_df = pd.DataFrame(np.append(net_df.iloc[:,:-1].to_numpy(), tree_df.to_numpy(), axis=1))\\n\\n# split into train_df, test_df:\\nTEST_SIZE = 0.2\\ncombined_train_df, combined_test_df = train_test_split(combined_data_df, test_size=TEST_SIZE)\\n\\n# calculate the input-dimension for the blender - minus one is for the label column:\\nblender_input_dim = combined_data_df.shape[1]-1\\n\\n# create dataloader from train_df and test_df:\\nBATCH_SIZE=64\\ntrain_ds = WineDataSet(combined_train_df)\\ntest_ds = WineDataSet(combined_test_df)\\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\\ntest_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# LOAD AND PREPARE DATA: DATA FRESH LOAD - NOT for stacking:\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# DATA FRESH LOAD - NOT for stacking:\n",
    "data_path = Path(\"./winequality-white.csv\")\n",
    "column_names = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \n",
    "\"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n",
    "#column_names_to_normalize = column_names[:-1]\n",
    "wine_df = pd.read_csv(data_path, header=0, names=column_names, sep=\";\")\n",
    "\n",
    "# prepare data:\n",
    "shuffle(wine_df, random_state=0)\n",
    "scaled_wine_df = scale_dataframe(wine_df, exempt_last_column=True)\n",
    "\n",
    "# names/ paths of the pre-trained classifiers:\n",
    "#WineNetwork_filename = \"model_640_0.001_369_64_SGD.pt\"\n",
    "NET_FILENAME=\"model_635_0.001_255_64_SGD.pt\"\n",
    "TREE_FILENAME= 'AdaBoost_071_model.dct'\n",
    "\n",
    "# use the scaled data for the net:\n",
    "net_df = WineNetwork_map(scaled_wine_df, net_name=NET_FILENAME)\n",
    "net_df = scale_dataframe(net_df)\n",
    "\n",
    "# use the unscaled data for the tree:\n",
    "tree_df = Wine_tree_map(wine_df, tree_name=TREE_FILENAME, does_predict_proba=True)\n",
    "tree_df = scale_dataframe(tree_df)\n",
    "\n",
    "# combine net_df and tree_df to combined_data_df:\n",
    "combined_data_df = pd.DataFrame(np.append(net_df.iloc[:,:-1].to_numpy(), tree_df.to_numpy(), axis=1))\n",
    "\n",
    "# split into train_df, test_df:\n",
    "TEST_SIZE = 0.2\n",
    "combined_train_df, combined_test_df = train_test_split(combined_data_df, test_size=TEST_SIZE)\n",
    "\n",
    "# calculate the input-dimension for the blender - minus one is for the label column:\n",
    "blender_input_dim = combined_data_df.shape[1]-1\n",
    "\n",
    "# create dataloader from train_df and test_df:\n",
    "BATCH_SIZE=64\n",
    "train_ds = WineDataSet(combined_train_df)\n",
    "test_ds = WineDataSet(combined_test_df)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data for Mapping and Blender Training: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note to the following code: <br>\n",
    "we have to pad the tree-output because the first 3 classes of \"quality\" are not present in the dataset, i.e. the  <br>\n",
    "tree outputs the probability of the i'th of the present classes, i.e. the i'th in 4,5,6,7,8,9 <br>\n",
    "Might also be easier for the blender in the end, to have the i'th input meaning the same for all inputs. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_train_df.shape: (3917, 11)\n",
      "tree_train_df.shape : (3917, 11)\n",
      "tree2_train_df.shape : (3917, 11)\n",
      "svm_train_df.shape : (3917, 11)\n",
      "BLENDER_INPUT_DIM : 40\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPARE DATA FOR MAPPING AND BLENDER TRAINING:\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# DATA LOAD FOR STACKING:\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# names/ paths of the pre-trained classifiers:\n",
    "NET_FILENAME = \"net_model_660_0.001_299_64_SGD.pt\"\n",
    "TREE_FILENAME = 'AdaBoost_0.713_0.6_460_model.dct'\n",
    "#TREE_FILENAME2 = \"DecisionTree_061_model.dct\"\n",
    "TREE_FILENAME2 = \"GradientBoost_0.693_0.05_200_model.dct\"\n",
    "SVM_FILENAME = \"svm_rbf_kernel_0.67_G1_C50.svm\"\n",
    "\n",
    "\n",
    "# ----- TRAIN DATA PREPARATION -----:\n",
    "\n",
    "# use predict_proba method for trees:\n",
    "predict_proba = True\n",
    "\n",
    "# use the scaled data for the net:\n",
    "scaled_train_df = scale_dataframe(train_df, exempt_last_column=True)\n",
    "us_net_train_df = WineNetwork_map(scaled_train_df, net_name=NET_FILENAME)\n",
    "net_train_df = scale_dataframe(us_net_train_df, exempt_last_column=True)\n",
    "print(f\"net_train_df.shape: {net_train_df.shape}\")\n",
    "\n",
    "# use the unscaled data for the tree:\n",
    "us_tree_train_df = Wine_tree_map(train_df, tree_name=TREE_FILENAME, does_predict_proba=predict_proba)\n",
    "# padding of the tree-output\n",
    "if(predict_proba):\n",
    "    us_tree_train_df = np.pad(us_tree_train_df, ((0,0), (3,0)), \"constant\", constant_values=0)\n",
    "tree_train_df = scale_dataframe(us_tree_train_df, exempt_last_column=True)\n",
    "print(f\"tree_train_df.shape : {tree_train_df.shape}\")\n",
    "\n",
    "us_tree2_train_df = Wine_tree_map(train_df, tree_name=TREE_FILENAME2, does_predict_proba=predict_proba)\n",
    "if(predict_proba):\n",
    "    us_tree2_train_df = np.pad(us_tree2_train_df, ((0,0), (3,0)), \"constant\", constant_values=0)\n",
    "tree2_train_df = scale_dataframe(us_tree2_train_df, exempt_last_column=True)\n",
    "print(f\"tree2_train_df.shape : {tree2_train_df.shape}\")\n",
    "\n",
    "# use the unscaled data for the svm (svm contains own scaler):\n",
    "us_svm_train_df = SVM_RBF_map(train_df, svm_filename=SVM_FILENAME)\n",
    "svm_train_df = scale_dataframe(us_svm_train_df, exempt_last_column=True)\n",
    "print(f\"svm_train_df.shape : {svm_train_df.shape}\")\n",
    "\n",
    "# combine net_train_df and tree_train_df to combined_train_:\n",
    "combined_train_ = np.append(net_train_df.iloc[:,:-1].to_numpy(), tree_train_df.iloc[:,:-1].to_numpy(), axis=1)\n",
    "# combine combined_train and svm_train_df:\n",
    "combined_train__ = np.append(combined_train_, svm_train_df.iloc[:,:-1].to_numpy(), axis=1)\n",
    "# combine combined_train__ and tree2_train_df -- the true labels are contained in tree2_train_df:\n",
    "combined_train_df = pd.DataFrame(np.append(combined_train__, tree2_train_df.to_numpy(), axis=1))\n",
    "\n",
    "\n",
    "# ----- TEST DATA PREPARATION -----:\n",
    "\n",
    "# use the scaled data for the net:\n",
    "scaled_test_df = scale_dataframe(test_df, exempt_last_column=True)\n",
    "us_net_test_df = WineNetwork_map(scaled_test_df, net_name=NET_FILENAME)\n",
    "net_test_df = scale_dataframe(us_net_test_df, exempt_last_column=True)\n",
    "\n",
    "# use the unscaled data for the tree:\n",
    "us_tree_test_df = Wine_tree_map(test_df, tree_name=TREE_FILENAME, does_predict_proba=predict_proba)\n",
    "# padding of the tree-output:\n",
    "if(predict_proba):\n",
    "    us_tree_test_df = np.pad(us_tree_test_df, ((0,0), (3,0)), \"constant\", constant_values=0)\n",
    "tree_test_df = scale_dataframe(us_tree_test_df, exempt_last_column=True)\n",
    "\n",
    "us_tree2_test_df = Wine_tree_map(test_df, tree_name=TREE_FILENAME2, does_predict_proba=predict_proba)\n",
    "if(predict_proba):\n",
    "    us_tree2_test_df = np.pad(us_tree2_test_df, ((0,0), (3,0)), \"constant\", constant_values=0)\n",
    "tree2_test_df = scale_dataframe(us_tree2_test_df, exempt_last_column=True)\n",
    "\n",
    "# use the unscaled data for the svm (svm contains own scaler):\n",
    "us_svm_test_df = SVM_RBF_map(test_df, svm_filename=SVM_FILENAME)\n",
    "svm_test_df = scale_dataframe(us_svm_test_df, exempt_last_column=True)\n",
    "\n",
    "# combine net_test_df and tree_test_df to combined_test_:\n",
    "combined_test_ = np.append(net_test_df.iloc[:,:-1].to_numpy(), tree_test_df.iloc[:,:-1].to_numpy(), axis=1)\n",
    "# combine combined_test_ and svm_test_df to combined_test__:\n",
    "combined_test__ = np.append(combined_test_, svm_test_df.iloc[:,:-1].to_numpy(), axis=1)\n",
    "# combine combined_test_ and tree2_test_df -- the true-labels are contained in tree2_test_df:\n",
    "combined_test_df = pd.DataFrame(np.append(combined_test__, tree2_test_df.to_numpy(), axis=1))\n",
    "\n",
    "\n",
    "# PARAMETERS:\n",
    "# calculate the input-dimension for the blender - minus one is for the label column:\n",
    "BLENDER_INPUT_DIM = combined_test_df.shape[1]-1\n",
    "print(f\"BLENDER_INPUT_DIM : {BLENDER_INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.923850</td>\n",
       "      <td>1.821698</td>\n",
       "      <td>-0.746288</td>\n",
       "      <td>-0.714692</td>\n",
       "      <td>-0.622271</td>\n",
       "      <td>-0.790963</td>\n",
       "      <td>-0.335404</td>\n",
       "      <td>-0.073447</td>\n",
       "      <td>0.496269</td>\n",
       "      <td>-0.744514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062002</td>\n",
       "      <td>-0.191108</td>\n",
       "      <td>1.532945</td>\n",
       "      <td>-0.902404</td>\n",
       "      <td>-0.462085</td>\n",
       "      <td>-0.192535</td>\n",
       "      <td>-0.035751</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.905917</td>\n",
       "      <td>-0.693538</td>\n",
       "      <td>0.525621</td>\n",
       "      <td>0.388909</td>\n",
       "      <td>0.141635</td>\n",
       "      <td>0.536781</td>\n",
       "      <td>0.304240</td>\n",
       "      <td>-1.067679</td>\n",
       "      <td>-0.076927</td>\n",
       "      <td>0.553048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>-0.191098</td>\n",
       "      <td>-0.651486</td>\n",
       "      <td>1.106016</td>\n",
       "      <td>-0.461937</td>\n",
       "      <td>-0.192519</td>\n",
       "      <td>-0.035750</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.168089</td>\n",
       "      <td>-0.717691</td>\n",
       "      <td>0.253501</td>\n",
       "      <td>0.152797</td>\n",
       "      <td>-0.021800</td>\n",
       "      <td>0.252715</td>\n",
       "      <td>0.777717</td>\n",
       "      <td>-0.814238</td>\n",
       "      <td>0.357285</td>\n",
       "      <td>0.275439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062003</td>\n",
       "      <td>-0.191117</td>\n",
       "      <td>-0.652551</td>\n",
       "      <td>-0.904433</td>\n",
       "      <td>2.165403</td>\n",
       "      <td>-0.192537</td>\n",
       "      <td>-0.035752</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.563537</td>\n",
       "      <td>0.542853</td>\n",
       "      <td>0.713536</td>\n",
       "      <td>0.551958</td>\n",
       "      <td>0.254496</td>\n",
       "      <td>0.732946</td>\n",
       "      <td>-0.008685</td>\n",
       "      <td>-1.500509</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.744753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062003</td>\n",
       "      <td>-0.191110</td>\n",
       "      <td>1.537196</td>\n",
       "      <td>-0.906210</td>\n",
       "      <td>-0.462218</td>\n",
       "      <td>-0.192537</td>\n",
       "      <td>-0.035751</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.238549</td>\n",
       "      <td>-0.488892</td>\n",
       "      <td>0.122626</td>\n",
       "      <td>0.039241</td>\n",
       "      <td>2.612338</td>\n",
       "      <td>0.116095</td>\n",
       "      <td>-0.299400</td>\n",
       "      <td>-0.495350</td>\n",
       "      <td>-0.365885</td>\n",
       "      <td>0.141925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.062002</td>\n",
       "      <td>-0.191098</td>\n",
       "      <td>-0.646207</td>\n",
       "      <td>1.101428</td>\n",
       "      <td>-0.462275</td>\n",
       "      <td>-0.192535</td>\n",
       "      <td>-0.035751</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.923850  1.821698 -0.746288 -0.714692 -0.622271 -0.790963 -0.335404   \n",
       "1  1.905917 -0.693538  0.525621  0.388909  0.141635  0.536781  0.304240   \n",
       "2  1.168089 -0.717691  0.253501  0.152797 -0.021800  0.252715  0.777717   \n",
       "3  0.563537  0.542853  0.713536  0.551958  0.254496  0.732946 -0.008685   \n",
       "4  0.238549 -0.488892  0.122626  0.039241  2.612338  0.116095 -0.299400   \n",
       "\n",
       "         7         8         9   ...   31   32        33        34        35  \\\n",
       "0 -0.073447  0.496269 -0.744514  ...  0.0  0.0 -0.062002 -0.191108  1.532945   \n",
       "1 -1.067679 -0.076927  0.553048  ...  0.0  0.0 -0.062000 -0.191098 -0.651486   \n",
       "2 -0.814238  0.357285  0.275439  ...  0.0  0.0 -0.062003 -0.191117 -0.652551   \n",
       "3 -1.500509  0.057813  0.744753  ...  0.0  0.0 -0.062003 -0.191110  1.537196   \n",
       "4 -0.495350 -0.365885  0.141925  ...  0.0  0.0 -0.062002 -0.191098 -0.646207   \n",
       "\n",
       "         36        37        38        39   40  \n",
       "0 -0.902404 -0.462085 -0.192535 -0.035751  5.0  \n",
       "1  1.106016 -0.461937 -0.192519 -0.035750  6.0  \n",
       "2 -0.904433  2.165403 -0.192537 -0.035752  7.0  \n",
       "3 -0.906210 -0.462218 -0.192537 -0.035751  5.0  \n",
       "4  1.101428 -0.462275 -0.192535 -0.035751  6.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Tree- and the Net-Classifier ###\n",
    "? How many of the instances that the net gets right, does the tree get right? <br>\n",
    "At first - compare on the test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ON TEST DATA ---\n",
      "X_tree_test.shape: (979, 10)\n",
      "accuracy count net: 349, i.e. 0.356\n",
      "accuracy count tree: 698, i.e. 0.713\n",
      "net and tree agree on 427 instances.\n",
      "The correct predictions of the net are those of the tree: False\n",
      "There are 292 correct predictions made by both simultaneously.\n",
      "There are 57 correct predictions made by the net only, that is 0.058 of the data.\n"
     ]
    }
   ],
   "source": [
    "# TEST-SET COMPARISON:\n",
    "# we have to use the un-scaled data of courses:\n",
    "\n",
    "# get the class predictions of the net classifier:\n",
    "X_net_test = us_net_test_df.iloc[:,:-1].to_numpy()\n",
    "y_pred_net_test = np.argmax(X_net_test, axis=1)\n",
    "\n",
    "# get the class predictions of the tree classifier:\n",
    "X_tree_test = us_tree_test_df[:, :-1]\n",
    "y_pred_tree_test = np.argmax(X_tree_test, axis=1)\n",
    "\n",
    "# get the true labels:\n",
    "#y_true_test = us_net_test_df.iloc[:,-1].to_numpy()\n",
    "y_true_test = us_tree_test_df[:,-1]\n",
    "nof_instances = len(y_true_test)\n",
    "accc_net = (y_pred_net_test == y_true_test).sum()\n",
    "accc_tree = (y_pred_tree_test.squeeze() == y_true_test).sum()\n",
    "ic_tree_net = (y_pred_net_test == y_pred_tree_test.squeeze()).sum()\n",
    "\n",
    "print(\"--- ON TEST DATA ---\")\n",
    "print(f\"X_tree_test.shape: {X_tree_test.shape}\")\n",
    "print(f\"accuracy count net: {accc_net}, i.e. {accc_net / nof_instances :.3f}\")\n",
    "print(f\"accuracy count tree: {accc_tree}, i.e. {accc_tree / nof_instances :.3f}\")\n",
    "print(f\"net and tree agree on {ic_tree_net} instances.\")\n",
    "\n",
    "# get the instances where the net is correct:\n",
    "true_net_preds_test = np.where(np.equal(y_pred_net_test, y_true_test))\n",
    "true_tree_preds_test = np.where(np.equal(y_pred_tree_test, y_true_test))\n",
    "# calculate where they are both correct:\n",
    "N = set(true_net_preds_test[0])\n",
    "T = set(true_tree_preds_test[0])\n",
    "print(f\"The correct predictions of the net are those of the tree: {N.issubset(T)}\")\n",
    "inters = len(N.intersection(T))\n",
    "print(f\"There are {inters} correct predictions made by both simultaneously.\")\n",
    "print(f\"There are {accc_net - inters} correct predictions made by the net only, that is {(accc_net - inters)/len(y_true_test) :.3f} of the data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on the test-set there are some true classifications made by the net-classifier, that are done wrong by the tree-classifier. <br>\n",
    "We would expect from this, that the ensemble resulting from stacking with a blender would be more accurate than each single classifier. <br>\n",
    "<br>\n",
    "To get a quick result, we just add the probability-masses of both predictors (soft-voting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy soft-voting: 0.713\n"
     ]
    }
   ],
   "source": [
    "# instead of training a blender, we take simply the sum of the probability-masses as prediction of the ensemble and renormalize:\n",
    "y_pred_test = X_tree_test + X_net_test\n",
    "y_pred_class_test = np.argmax(y_pred_test, axis=1)\n",
    "print(f\"Accuracy soft-voting: {(y_pred_class_test == y_true_test).sum()/ len(y_true_test) :.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the soft-voting we have a predictor with 71% accuracy - identical to the AdaBoosted Decision-Tree, i.e. *this soft-voting blender is not improving the overall classification.* <br>\n",
    "<br>\n",
    "So let's TRAIN a blender instead of PRESCRIBING one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Stacked Ensemble with a Blender:\n",
    "Create the DataLoader from the DataFrames/ DataSets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader from train_df and test_df:\n",
    "BATCH_SIZE=512\n",
    "train_ds = WineDataSet(combined_train_df)\n",
    "test_ds = WineDataSet(combined_test_df)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Blender Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BlenderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(input_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(256, 10),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epochs: 700, Learning-rate : 0.001, Optimizer : SGD, Batch-Size : 512\n",
      "\n",
      "----- Epoch: 100 -----\n",
      "Epoch loss: 0.003838547525415625\n",
      "Epoch accuracy: 0.7088866189989785\n",
      "\n",
      "----- Epoch: 200 -----\n",
      "Epoch loss: 0.004148044308554286\n",
      "Epoch accuracy: 0.710929519918284\n",
      "\n",
      "----- Epoch: 300 -----\n",
      "Epoch loss: 0.0043662254364668284\n",
      "Epoch accuracy: 0.7099080694586313\n",
      "\n",
      "----- Epoch: 400 -----\n",
      "Epoch loss: 0.004504983588307335\n",
      "Epoch accuracy: 0.7099080694586313\n",
      "\n",
      "----- Epoch: 500 -----\n",
      "Epoch loss: 0.004605417962702594\n",
      "Epoch accuracy: 0.7099080694586313\n",
      "\n",
      "----- Epoch: 600 -----\n",
      "Epoch loss: 0.004706825737568404\n",
      "Epoch accuracy: 0.7099080694586313\n",
      "\n",
      "----- Epoch: 700 -----\n",
      "Epoch loss: 0.004780921069058505\n",
      "Epoch accuracy: 0.7099080694586313\n"
     ]
    }
   ],
   "source": [
    "# Train the blender model:\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# writer for tensorboard:\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# blender model and parameters:\n",
    "\n",
    "# TODO: ACTUALLY USE CUDA IF AVAILABLE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "blender_model = BlenderModel(BLENDER_INPUT_DIM)\n",
    "\n",
    "# parameters:\n",
    "\n",
    "# loss function:\n",
    "# cross-entropy:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer:\n",
    "\n",
    "# adam:\n",
    "#OPTIMIZER_NAME = \"ADAM\"\n",
    "#LEARNING_RATE = 1e-4\n",
    "#optimizer = torch.optim.Adam(blender_model.parameters(), lr=LEARNING_RATE, weight_decay=0.5, amsgrad=True)\n",
    "#optimizer = torch.optim.Adam(blender_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# sgd:\n",
    "OPTIMIZER_NAME = \"SGD\"\n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM=0.9\n",
    "optimizer = torch.optim.SGD(blender_model.parameters(), lr= LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# adamax:\n",
    "#OPTIMIZER_NAME = \"ADAMAX\"\n",
    "#LEARNING_RATE = 1e-4\n",
    "#optimizer = torch.optim.Adamax(blender_model.parameters(), lr=LEARNING_RATE, weight_decay=0.8)\n",
    "\n",
    "# LBFGS\n",
    "#OPTIMIZER_NAME = \"LBFGFS\"\n",
    "#LEARNING_RATE = 1\n",
    "#MAX_ITER = 20\n",
    "#optimizer = torch.optim.LBFGS(params, lr=LEARNING_RATE, max_iter=MAX_ITER, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "\n",
    "\n",
    "EPOCHS = 700\n",
    "WRITE_LOG_AFTER_EPOCHS = 100\n",
    "best_model_name = \"\"\n",
    "max_correct = float(\"-inf\")\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}, Learning-rate : {LEARNING_RATE}, Optimizer : {OPTIMIZER_NAME}, Batch-Size : {BATCH_SIZE}\")\n",
    "for ep in range(1, EPOCHS+1):        \n",
    "       \n",
    "        # put model in train mode:\n",
    "        blender_model.train()\n",
    "        (train_loss, train_no_correct) = train_loop(train_dl, blender_model, loss_fn, optimizer)\n",
    "              \n",
    "        # switch model to to evaluation mode:\n",
    "        blender_model.eval()\n",
    "        (test_loss, test_no_correct) = test_loop(test_dl, blender_model, loss_fn)\n",
    "\n",
    "        if(test_no_correct > max_correct):\n",
    "            max_correct = test_no_correct\n",
    "            #if(best_model_name):\n",
    "            #    os.remove(best_model_name)\n",
    "            best_model_name = \"./blender_model_\" + str(test_no_correct) + \"_\" + str(LEARNING_RATE) + \"_\" + str(ep) + \"_\" + str(BATCH_SIZE) + \"_\" + OPTIMIZER_NAME + \"_sv.pt\"\n",
    "            # torch.save(blender_model.state_dict(), best_model_name)\n",
    "            best_dict = copy.deepcopy(blender_model.state_dict())\n",
    "\n",
    "        writer.add_scalar(\"Loss/test\", test_loss/ len(test_ds), ep)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_no_correct/ len(test_ds), ep)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss/ len(train_ds), global_step=ep)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_no_correct/ len(train_ds), global_step=ep)\n",
    "       \n",
    "        if ep % WRITE_LOG_AFTER_EPOCHS == 0:\n",
    "            print(f\"\\n----- Epoch: {ep} -----\")\n",
    "            print(f\"Epoch loss: {test_loss/ len(test_ds)}\")\n",
    "            print(f\"Epoch accuracy: {test_no_correct/ len(test_ds)}\")\n",
    "\n",
    "torch.save(best_dict, best_model_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test statistics: ####\n",
    "Names of Models are constructed according to: <br>\n",
    "best_model_name = \"./blender_model_\" + str(test_no_correct) + \"_\" + str(LEARNING_RATE) + \"_\" + str(ep) + \"_\" + str(BATCH_SIZE) + \"_\" + OPTIMIZER_NAME + \"_sv.pt <br>\n",
    "<br>\n",
    "1. blender_model_693_0.0001_48_256_SGD_sv.pt - 70.7% accuracy <br>\n",
    "2. blender_model_696_0.0001_15_512_ADAM_sv.pt - 71.0% accuracy <br>\n",
    "3. blender_model_696_1e-05_219_1024_ADAM_sv.pt - 71.0% accuracy <br>\n",
    "4. blender_model_697_0.0001_16_512_ADAM_sv.pt - 71.1% accuracy <br>\n",
    "5. blender_model_697_0.0001_44_256_SGD_sv.pt - 71.1% accuracy <br>\n",
    "6. blender_model_698_0.0001_11_256_ADAM_sv.pt - 71.2% accuracy <br>\n",
    "7. blender_model_698_1e-05_100_512_ADAM_sv.pt - 71.2% accuracy <br>\n",
    "8. blender_model_701_0.0001_36_256_SGD_sv.pt - 71.5% accuracy <br>\n",
    "9. blender_model_701_1e-05_192_512_ADAM_sv.pt - 71.5% accuracy <br>\n",
    "10. blender_model_697_0.001_13_256_ADAMAX_sv.pt - 71.1% accuracy <br>\n",
    "11. blender_model_706_0.0001_346_512_ADAM_sv.pt - 72.0% accuracy <br>\n",
    "<br>\n",
    "The blender was feed by the following three models: <br>\n",
    "<br>\n",
    "AdaBoost_071_model.dct - 71% accuracy<br>\n",
    "DecisionTree_061_model.dct - 61% accuracy<br>\n",
    "net_model_647_0.001_393_64_SGD.pt - 64% accuracy<br>\n",
    "(except for model number 11. which was feed by net_model_660_0.001_299_64_SGD.pt - 71,8% accuracy instead.) <br>\n",
    "<br>\n",
    "Summary: <br>\n",
    "I would have expected the blender to be considerably better than the best of the in-going predictors (after all the net does 5% of the data correct, that the Adaboost gets wrong), especially since the net and the decision-tree are so much different models (i.e. different by their construction, but maybe not by their prediction behaviour... needs to be investigated...)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation on the failing stacking-blender setup ##\n",
    "### Compare the Tree- and the Net-Classifier ###\n",
    "Above in this notebook, we already compared the net- and the adaboost-tree-classifier on the test data. <br>\n",
    "Here we do the same on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ON TRAIN DATA ---\n",
      "accuracy count net: 1357, i.e. 0.346\n",
      "accuracy count tree: 3917, i.e. 1.000\n",
      "net and tree agree on 1357 instances.\n",
      "The correct predictions of the net are those of the tree: True\n",
      "There are 1357 correct predictions made by both simultaneously\n",
      "There are 0 correct predictions made by the net only, that is 0.000 of the data.\n"
     ]
    }
   ],
   "source": [
    "# COMPARING ON THE TRAIN SET:\n",
    "\n",
    "# we have to use the un-scaled (us) data of course:\n",
    "\n",
    "# get the class predictions of the net classifier:\n",
    "X_net = us_net_train_df.iloc[:,:-1].to_numpy()\n",
    "y_pred_net = np.argmax(X_net, axis=1)\n",
    "\n",
    "# get the class predictions of the tree classifier:\n",
    "X_tree = us_tree_train_df[:, :-1]\n",
    "y_pred_tree = np.argmax(X_tree, axis=1) #+ 3 # <--- plus 3 because the first 3 classes do not appear in the set.\n",
    "\n",
    "# get the true labels:\n",
    "y_true = us_net_train_df.iloc[:,-1].to_numpy()\n",
    "nof_instances = len(y_true)\n",
    "accc_net = (y_pred_net == y_true).sum()\n",
    "accc_tree = (y_pred_tree.squeeze() == y_true).sum()\n",
    "ic_tree_net = (y_pred_net == y_pred_tree.squeeze()).sum()\n",
    "\n",
    "\n",
    "print(\"--- ON TRAIN DATA ---\")\n",
    "print(f\"accuracy count net: {accc_net}, i.e. {accc_net / nof_instances :.3f}\")\n",
    "print(f\"accuracy count tree: {accc_tree}, i.e. {accc_tree / nof_instances :.3f}\")\n",
    "print(f\"net and tree agree on {ic_tree_net} instances.\")\n",
    "\n",
    "# get the instances where the net is correct:\n",
    "true_net_preds = np.where(np.equal(y_pred_net, y_true))\n",
    "true_tree_preds = np.where(np.equal(y_pred_tree, y_true))\n",
    "# calculate where they are both correct:\n",
    "N = set(true_net_preds[0])\n",
    "T = set(true_tree_preds[0])\n",
    "print(f\"The correct predictions of the net are those of the tree: {N.issubset(T)}\")\n",
    "inters = len(N.intersection(T))\n",
    "print(f\"There are {inters} correct predictions made by both simultaneously\")\n",
    "print(f\"There are {accc_net - inters} correct predictions made by the net only, that is {(accc_net - inters)/len(y_true) :.3f} of the data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the training-set show: the tree (in this case the AdaBoost only) is overfitting on the train set. <br>\n",
    "That means that the input to the blender comming from the Adaboost-Tree is equal to the desired output. <br>\n",
    "<br>\n",
    "The blender has nothing to learn, the only thing it has to do, is to learn to ignore the input comming <br> \n",
    "from the net-classifier and having done so, the entire ensemble-stacking-blender setup is just the Adaboost-Tree.\n",
    "\n",
    "### Conclusion: ###\n",
    "There is no point in training a stacked ensemble with a blender, when one of the in-going classifiers is overfitting badly. <br>\n",
    "<br>\n",
    "What can be done now?:\n",
    "1. would the ensemble actually become better when using a non-overfitting, perhabs slightly worse Adaboost-Tree?\n",
    "2. can we improve by implementing a \"drop-out\" blender training?: replace s% of the training input comming from the AdaboostTree by <br>\n",
    "    a copy of the input from the net-classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a \"Drop-Out\" Blender ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to do something drop_out% of the time\n",
    "from torch.distributions import Uniform\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, drop_out=0):\n",
    "    losses, nof_correct = 0, 0\n",
    "    ud = Uniform(torch.tensor(0.0), torch.tensor(1.0))\n",
    "    for xx, y_true in dataloader:\n",
    "        if(model.training and drop_out != 0):\n",
    "            s = ud.sample()\n",
    "            if(s <= drop_out):\n",
    "                # normal data layout: net, tree, svm, tree2 with 10 columns each\n",
    "                # without svm:\n",
    "                #xx = torch.cat((xx[:,:10], xx[:,:10], xx[:,20:]), axis=1) # net-net-tree2\n",
    "                #xx = torch.cat((xx[:, 10: 20], xx[:,10:]), axis=1) # tree-tree-tree2\n",
    "                #xx = torch.cat((xx[:,-10:], xx[:,10:]), axis=1) # tree2-tree-tree2\n",
    "                #xx = torch.cat((xx[:, 10:20], xx[:, 10:20], xx[:, 10:20]), axis=1) # tree-tree-tree\n",
    "\n",
    "                # including svm:\n",
    "                #xx = torch.cat((xx[:,:10], xx[:,:10], xx[:,20:]), axis=1) # net-net-svm-tree2\n",
    "                #xx = torch.cat((xx[:, 10: 20], xx[:,10:]), axis=1) # tree-tree-svm-tree2\n",
    "                xx = torch.cat((xx[:,:10], xx[:,30:], xx[:,20:]), axis=1) # net-tree2-svm-tree2\n",
    "                \n",
    "        y_pred = model(xx)\n",
    "        loss= loss_fn(y_pred, y_true)\n",
    "        losses += loss.item()\n",
    "        nof_correct += (y_pred.argmax(1) == y_true).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses, nof_correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    losses, no_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (X,y_true) in dataloader:\n",
    "            pred = model(X)\n",
    "            losses += loss_fn(pred, y_true).item()\n",
    "            no_correct += (pred.argmax(1)== y_true).sum().item()\n",
    "     \n",
    "    return losses, no_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epochs: 600, Learning-rate : 0.0001, Optimizer : ADAM, Batch-Size : 512, Blender-Drop-Out : 0.3\n",
      "\n",
      "----- Epoch: 100 -----\n",
      "Epoch loss: 0.002849455021009747\n",
      "Epoch accuracy: 0.7017364657814096\n",
      "\n",
      "----- Epoch: 200 -----\n",
      "Epoch loss: 0.00312253659299981\n",
      "Epoch accuracy: 0.7007150153217568\n",
      "\n",
      "----- Epoch: 300 -----\n",
      "Epoch loss: 0.003307237566674206\n",
      "Epoch accuracy: 0.7017364657814096\n",
      "\n",
      "----- Epoch: 400 -----\n",
      "Epoch loss: 0.00339957642482177\n",
      "Epoch accuracy: 0.7027579162410623\n",
      "\n",
      "----- Epoch: 500 -----\n",
      "Epoch loss: 0.003514115973561241\n",
      "Epoch accuracy: 0.7007150153217568\n",
      "\n",
      "----- Epoch: 600 -----\n",
      "Epoch loss: 0.003598435541217247\n",
      "Epoch accuracy: 0.7017364657814096\n"
     ]
    }
   ],
   "source": [
    "# Train a drop-out blender model:\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# writer for tensorboard:\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# blender model and parameters:\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "blender_model = BlenderModel(BLENDER_INPUT_DIM)\n",
    "\n",
    "# parameters:\n",
    "\n",
    "# loss function:\n",
    "# cross-entropy:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer:\n",
    "\n",
    "# adam:\n",
    "OPTIMIZER_NAME = \"ADAM\"\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = torch.optim.Adam(blender_model.parameters(), lr=LEARNING_RATE, weight_decay=0.5, amsgrad=True)\n",
    "optimizer = torch.optim.Adam(blender_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# sgd:\n",
    "#OPTIMIZER_NAME = \"SGD\"\n",
    "#LEARNING_RATE = 1e-4\n",
    "#MOMENTUM=0.9\n",
    "optimizer = torch.optim.SGD(blender_model.parameters(), lr= LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# adamax:\n",
    "#OPTIMIZER_NAME = \"ADAMAX\"\n",
    "#LEARNING_RATE = 1e-4\n",
    "#optimizer = torch.optim.Adamax(blender_model.parameters(), lr=LEARNING_RATE, weight_decay=0.8)\n",
    "\n",
    "# LBFGS\n",
    "#OPTIMIZER_NAME = \"LBFGFS\"\n",
    "#LEARNING_RATE = 1\n",
    "#MAX_ITER = 20\n",
    "#optimizer = torch.optim.LBFGS(params, lr=LEARNING_RATE, max_iter=MAX_ITER, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "\n",
    "\n",
    "EPOCHS = 600\n",
    "WRITE_LOG_AFTER_EPOCHS = 100\n",
    "BLENDER_DROP_OUT = 0.3\n",
    "best_model_name = \"\"\n",
    "max_correct = float(\"-inf\")\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}, Learning-rate : {LEARNING_RATE}, Optimizer : {OPTIMIZER_NAME}, Batch-Size : {BATCH_SIZE}, Blender-Drop-Out : {BLENDER_DROP_OUT}\")\n",
    "for ep in range(1, EPOCHS+1):        \n",
    "       \n",
    "        # put model in train mode:\n",
    "        blender_model.train()\n",
    "        (train_loss, train_no_correct) = train_loop(train_dl, blender_model, loss_fn, optimizer, drop_out=BLENDER_DROP_OUT)\n",
    "              \n",
    "        # switch model to to evaluation mode:\n",
    "        blender_model.eval()\n",
    "        (test_loss, test_no_correct) = test_loop(test_dl, blender_model, loss_fn)\n",
    "\n",
    "        if(test_no_correct > max_correct):\n",
    "            max_correct = test_no_correct\n",
    "            #if(best_model_name):\n",
    "            #    os.remove(best_model_name)\n",
    "            best_model_name = \"./blender_model_\" + str(test_no_correct) + \"_\" + str(LEARNING_RATE) + \"_\" + str(ep) + \"_\" + str(BATCH_SIZE) + \"_\" + OPTIMIZER_NAME + \"_\" + str(BLENDER_DROP_OUT) + \"_sv.pt\"\n",
    "            # torch.save(blender_model.state_dict(), best_model_name)\n",
    "            best_dict = copy.deepcopy(blender_model.state_dict())\n",
    "\n",
    "        writer.add_scalar(\"Loss/test\", test_loss/ len(test_ds), ep)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_no_correct/ len(test_ds), ep)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss/ len(train_ds), global_step=ep)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_no_correct/ len(train_ds), global_step=ep)\n",
    "       \n",
    "        if ep % WRITE_LOG_AFTER_EPOCHS == 0:\n",
    "            print(f\"\\n----- Epoch: {ep} -----\")\n",
    "            print(f\"Epoch loss: {test_loss/ len(test_ds)}\")\n",
    "            print(f\"Epoch accuracy: {test_no_correct/ len(test_ds)}\")\n",
    "\n",
    "torch.save(best_dict, best_model_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: ###\n",
    "Blender training with drop-out does not improve the accuracy. The best model was trained with 702 correctly classified instances - which is a little worse than the best result obtained with the no drop-out training. <br>\n",
    "Interesstingly the best results were obtained not, as previously assumed, when we dropped the Adaboost-tree inputs, but when we dropped the net-inputs. <br>\n",
    "<br>\n",
    "It seems, though, that it is not possible, by using this approach, to add the roughly 5% of correctly classified samples the net is predicting outside the samples correctly classified by the Adaboosttree to the ensemble-predictor.\n",
    "<br>\n",
    "Latest addition to the blender-stack is an support vector machine - the results did not improve with this either: no improvement without blender-drop-out and no improvement with blender-drop-out. The svm allone is able to produce 67% accuracy. The ensemble/ blender improves this to about 70% - which is a nice gain, but still not better than the adaboosted tree alone.<br>\n",
    "<br>\n",
    "Training with blender-drop-out and svm in the stack - example result: <br>\n",
    "Epochs: 600, Learning-rate : 0.0001, Optimizer : ADAM, Batch-Size : 512, Blender-Drop-Out : 0.3 <br>\n",
    "<br>\n",
    "Blender-feed, while dropout: <br>\n",
    "xx = torch.cat((xx[:,:10], xx[:,30:], xx[:,20:]), axis=1) # net-tree2-svm-tree2 <br>\n",
    "accuracy: 0.704 <br>\n",
    "time: 44.1s"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63c4844b7ff1ba2a0ea33a2a697d60509a196ce3d2a18c44c2b4fb054ba492a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('conenv_fileassociation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
