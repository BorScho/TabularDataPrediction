{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-Tree Classifiers and their Ensembles for Classification of Wine-Quality ##\n",
    "Wine data-set downloaded from [csv-file](http://mng.bz/90Ol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data and Get an Overview: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_df.shape : (4898, 12)\n",
      "Number of unique 'qualities': 7\n",
      "Qualities: [3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "Class counts: [  20  163 1457 2198  880  175    5]\n",
      "\n",
      "Number of instances: 4898 \n",
      "\n",
      "Class fractions: [ 0.    0.    0.    0.41  3.33 29.75 44.88 17.97  3.57  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"./winequality-white.csv\")\n",
    "field_names = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \n",
    "\"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n",
    "\n",
    "wine_df = pd.read_csv(data_path, header=0, names=field_names, sep=\";\")\n",
    "\n",
    "# data overview:\n",
    "#print(wine_df.describe())\n",
    "\n",
    "# which quality classes do we have? :\n",
    "print(f\"wine_df.shape : {wine_df.shape}\")\n",
    "qualities = wine_df[\"quality\"].unique()\n",
    "print(f\"Number of unique 'qualities': {len(qualities)}\")\n",
    "print(f\"Qualities: {sorted(qualities)}\")\n",
    "binc = np.bincount([q for q in wine_df[\"quality\"]])\n",
    "no_inst = len(wine_df)\n",
    "print(f\"\\nClass counts: {binc[-len(qualities):]}\")\n",
    "print(f\"\\nNumber of instances: {no_inst} \")\n",
    "print(f\"\\nClass fractions: {np.round(binc/no_inst,4) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train- and Test-Set - Save those to Disk: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3918, 11)\n",
      "(3918, 11)\n",
      "(3918, 12)\n",
      "(980, 12)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#IF YOU EXECUTE THIS CODE THE MODELS OF THE STACK HAVE TO BE RETRAINED !!!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = wine_df.iloc[:,:-1]\n",
    "y = wine_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "X_train = X_train.to_numpy()\n",
    "print(X_train.shape)\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# write to disk for later use:\n",
    "#Z = pd.DataFrame(np.append(X_train, np.expand_dims(y_train, axis=1 ), axis=1))\n",
    "#Z.to_csv(\"./train.csv\", index=False)\n",
    "Z = np.append(X_train, np.expand_dims(y_train, axis=1 ), axis=1)\n",
    "print(Z.shape)\n",
    "np.savetxt(\"./train.csv\", Z, delimiter=\",\")\n",
    "#Z = pd.DataFrame(np.append(X_test, np.expand_dims(y_test, axis=1), axis=1))\n",
    "#Z.to_csv(\"./test.csv\", index=False)\n",
    "Z = np.append(X_test, np.expand_dims(y_test, axis=1), axis=1)\n",
    "print(Z.shape)\n",
    "np.savetxt(\"./test.csv\", Z, delimiter=\",\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Train- / Test-Data from Disk (without Pandas version): ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape : (3918,)\n",
      "X_train.shape : (3918, 11)\n",
      "X_test.shape : (980, 11)\n"
     ]
    }
   ],
   "source": [
    "# load data from the separate csv files - for stack- and blender training\n",
    "\n",
    "X_train = np.loadtxt(\"./train.csv\", delimiter=\",\")\n",
    "y_train = X_train[:,-1]\n",
    "print(f\"y_train.shape : {y_train.shape}\")\n",
    "X_train = X_train[:,:-1]\n",
    "print(f\"X_train.shape : {X_train.shape}\")\n",
    "X_test = np.loadtxt(\"./test.csv\", delimiter=\",\")\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]\n",
    "print(f\"X_test.shape : {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Simple Decision-Tree Classifier - Check it's Accuracy: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "acc: 0.6163265306122448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=20)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "print(y_pred[0])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"acc: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Decision-Tree-Classifier for Use in Stacked Voting: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Decision-Tree-Classifier for use in Stacked Voting:\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'DecisionTree_061_model.dct'\n",
    "pickle.dump(tree_clf, open(filename, 'wb'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the Model is Reloadable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "# Check if the model is reloadable:\n",
    "\n",
    "# load the model from disk\n",
    "tree_clf_reloaded = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# apply the reloaded model for inference:\n",
    "result = tree_clf_reloaded.score(X_test, y_test)\n",
    "print(f\"{result:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6163265306122448, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the accuracy by hand: \n",
    "\n",
    "y_pred = tree_clf_reloaded.predict(X_test)\n",
    "y_pred_prob = tree_clf_reloaded.predict_proba(X_test)\n",
    "y_pred_prob = np.argmax(y_pred_prob, axis=1) + 3 # <--- The classes 0,1,2 do not appear in the data! we have to add this as a \"bias\"\n",
    "(y_pred_prob == y_test).sum()/len(y_test), (y_pred_prob == y_pred).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an AdaBoost Classifier with a Decision-Tree-Classfier: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 1.00\n",
      "test acc: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=600, \n",
    "    algorithm=\"SAMME.R\",\n",
    "    learning_rate=0.6\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print(f\"train acc: {ada_clf.score(X_train, y_train):.2f}\")\n",
    "print(f\"test acc: {ada_clf.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the AdaBoost Classifier and Check if it is Reloadable: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "# save the AdaBoostClassifier:\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'AdaBoost_071_model.dct'\n",
    "pickle.dump(ada_clf, open(filename, 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "ada_clf_reloaded = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# apply the reloaded model for inference:\n",
    "result = ada_clf_reloaded.score(X_test, y_test)\n",
    "print(f\"{result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassifier( <br>\n",
    "    DecisionTreeClassifier(max_depth=10), <br>\n",
    "    n_estimators=600, <br>\n",
    "    algorithm=\"SAMME.R\", <br>\n",
    "    learning_rate=0.6 <br>\n",
    ") <br>\n",
    "<br>\n",
    "running-time: 25.7s <br>\n",
    "accuracy: 0.7160 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Bagging-Classifier with a Decision-Tree Classifier: ###\n",
    "Compare Accuracy and Out-Of-Bag-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators = 500,\n",
    "    max_samples =2800,\n",
    "    bootstrap = True,\n",
    "    n_jobs = -1,\n",
    "    oob_score = True,\n",
    "    bootstrap_features=True,\n",
    "    max_features = 0.8\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(f\"pred accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"bag_clf.oob_score_ : {bag_clf.oob_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier( <br>\n",
    "    DecisionTreeClassifier(), <br>\n",
    "    n_estimators = 500, <br>\n",
    "    max_samples =2800, <br>\n",
    "    bootstrap = True, <br>\n",
    "    n_jobs = -1, <br>\n",
    "    oob_score = True, <br>\n",
    "    bootstrap_features=True, <br>\n",
    "    max_features = 0.8 <br>\n",
    ") <br>\n",
    " <br>\n",
    "pred accuracy: 0.70 <br>\n",
    "bag_clf.oob_score_ : 0.68 <br>\n",
    "running time: 6.7s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random-Forrest Classifier: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_samples=2700,\n",
    ")\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred=rnd_clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"pred acc: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier( <br>\n",
    "    n_estimators=500, <br>\n",
    "    max_samples=2700, <br>\n",
    ") <br>\n",
    " <br>\n",
    "pred acc: 0.71 <br>\n",
    "running time: 5.2s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a Look at the Importance of the Features: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the importance of the features:\n",
    "\n",
    "feature_names = field_names[:-1]\n",
    "for name, scores in zip(feature_names, rnd_clf.feature_importances_):\n",
    "    print(f\"{name} : {scores:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GradientBoosting Classifier: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=120\n",
    ")\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print(f\"train scores: {accuracy_score(gb_clf.predict(X_train), y_train):.2f}\")\n",
    "print(f\"test scores: {accuracy_score(gb_clf.predict(X_test), y_test):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the best Number of Trees in the Forrest: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error # mse for CLASSIFICATION ?????\n",
    "import copy\n",
    "\n",
    "gbc = {}\n",
    "best_gbc = {}\n",
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=20,\n",
    "    subsample=0.5, \n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=10,\n",
    "    learning_rate=0.05,\n",
    "    warm_start=True, \n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "val_errors = []\n",
    "min_val_error=float(\"inf\")\n",
    "for n_estimators in range(100,300):\n",
    "    gbc.n_estimators=n_estimators\n",
    "    gbc.fit(X_train, y_train)\n",
    "    y_pred=gbc.predict(X_test)\n",
    "    val_error = accuracy_score(y_pred, y_test)\n",
    "    val_errors.append(val_error)\n",
    "    if(val_error < min_val_error):\n",
    "        min_val_error = val_error\n",
    "        best_gbc=copy.deepcopy(gbc) # deep-copy otherwize further training will spoil the classifier\n",
    "\n",
    "print(f\"train acc: {accuracy_score(y_pred=best_gbc.predict(X_train), y_true=y_train):.2f}\")\n",
    "print(f\"test acc: {accuracy_score(y_pred=best_gbc.predict(X_test), y_true=y_test):.2f}\")\n",
    "print(best_gbc)\n",
    "#print(val_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train acc: 0.9975 <br>\n",
    "test acc: 0.7048997772828508 <br>\n",
    "GradientBoostingClassifier(max_depth=20, n_estimators=63, subsample=0.5,\n",
    "                           warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train acc: 1.00 <br>\n",
    "test acc: 0.65 <br>\n",
    "GradientBoostingClassifier(max_depth=20, min_samples_leaf=10, <br>\n",
    "                           min_samples_split=500, n_estimators=995, <br>\n",
    "                           subsample=0.5, warm_start=True) <br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63c4844b7ff1ba2a0ea33a2a697d60509a196ce3d2a18c44c2b4fb054ba492a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('conenv_fileassociation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
